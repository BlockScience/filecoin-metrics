{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA for Gas Fees - V2\n",
    "\n",
    "In this notebook we perform Exploratory Data Analysis (EDA) on FIL's gas fee mechanism. The goal is to observe the gas fee as a signal and attempt to understand what may be driving it. \n",
    "\n",
    "Questions to be answered in this notebook:\n",
    "* Understand data we have by performing a data baseline - event and aggregated data.\n",
    "* Reverse engineer June 3 spike - what happened and why? Can we generalize what drives burning?\n",
    "* Distribution of attributes of things people are willing to pay gas fees for. What types of transactions are individuals paying more for?\n",
    "* Are revenues over cost?\n",
    "* Create curated data signals for Digital Twin use\n",
    "* Obtain message metadata, try and predict cost from it (classifier or regressor). Which attributes are mapped to smaller or larger costs? Which messages are worth paying a lot for? The question is what people are they paying for and *why*?\n",
    "* Reperform Granger Causality when we have more data understanding\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Background information: What are Gas Fees? \n",
    "Note: this description is copied from the [official Filecoin documentation](https://docs.filecoin.io/about-filecoin/how-filecoin-works/#gas-fees)\n",
    "\n",
    "Executing messages, for example by including transactions or proofs in the chain, consumes both computation and storage resources on the network. Gas is a measure of resources consumed by messages. The gas consumed by a message directly affects the cost that the sender has to pay for it to be included in a new block by a miner.\n",
    "\n",
    "Historically in other blockchains, miners specify a GasFee in a unit of native currency and then pay the block producing miners a priority fee based on how much gas is consumed by the message. Filecoin works similarly, except an amount of the fees is burned (sent to an irrecoverable address) to compensate for the network expenditure of resources, since all nodes need to validate the messages. The idea is based on Ethereum's EIP1559.\n",
    "\n",
    "The amount of fees burned in the Filecoin network comes given by a dynamic BaseFee which gets automatically adjusted according to the network congestion parameters (block sizes). The current value can be obtained from one of the block explorers or by inspecting the current head.\n",
    "\n",
    "Additionally, a number of gas-related parameters are attached to each message and determine the amount of rewards that miners get. Here's an overview of the terms and concepts:\n",
    "\n",
    "**GasUsage**: the amount of gas that a message's execution actually consumes. Current protocol does not know how much gas a message will exactly consume ahead of execution, but it can be estimated (see prices (opens new window)). GasUsage measured in units of Gas.\n",
    "\n",
    "**BaseFee**: the amount of FIL that gets burned per unit of gas consumed for the execution of every message. It is measured in units of attoFIL/Gas.\n",
    "\n",
    "**GasLimit**: the limit on the amount of gas that a message's execution can consume, estimated and specified by a message sender. It is measured in units of Gas. The sum of GasLimit for all messages included in a block must not exceed the BlockGasLimit. Messages will fail to execute if they run out of Gas, and any effects of the execution will be reverted.\n",
    "\n",
    "**GasFeeCap**: the maximum token amount that a sender is willing to pay per GasUnit for including a message in a block. It is measured in units of attoFIL/Gas. A message sender must have a minimum balance of GasFeeCap * GasLimit when sending a message, even though not all of that will be consumed. GasFeeCap can serve as a safeguard against high, unexpected BaseFee fluctuations.\n",
    "\n",
    "**GasPremium**: a priority fee that is paid to the block-producing miner. This is capped by GasFeeCap. The BaseFee has a higher priority. It is measured in units of attoFIL/Gas and can be as low as 1 attoFIL/Gas.\n",
    "\n",
    "**Overestimation burn**: an additional amount of gas to burn that grows larger when the difference between GasLimit and GasUsage is large. \n",
    "\n",
    "The total cost of a message for a sender will be:\n",
    "\n",
    "* GasUsage * BaseFee FIL (burned) +\n",
    "* GasLimit * GasPremium FIL (miner's reward) +\n",
    "* OverEstimationBurn * BaseFee FIL\n",
    "\n",
    "An important detail is that a message will always pay the burn fee, regardless of the GasFeeCap used. Thus, a low GasFeeCap may result in a reduced GasPremium or even a negative one! In that case, the miners that include a message will have to pay the needed amounts out of their own pockets, which means they are unlikely to include such messages in new blocks.\n",
    "\n",
    "Filecoin implementations may choose the heuristics of how their miners select messages for inclusion in new blocks, but they will usually attempt to maximize the miner's rewards.\n",
    "\n",
    "\n",
    "## Data Resources\n",
    "* https://hackmd.io/5leND7tITX--5NJsFCJyow # Existing Sentinel queries \n",
    "* https://github.com/filecoin-project/sentinel/blob/master/docs/sentinel_tables_purpose.csv # Sentinel Tables Purpose\n",
    "\n",
    "\n",
    "### Sentinel Diagram \n",
    "![](sentinel_diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "We are performing a vector autoregression (VAR) to determine if a casual or multi-causal relationship exists between the gas signals moves. This will help us understand the system-level dynamics of Filecoin.  \n",
    "\n",
    "### Vector Autoregression \n",
    "\n",
    "Vector autoregression (VAR) is a type of statistical model used to capture the relationship between multiple time series signals as they change over time. VAR models are extensions of univariate autoregression models allowing for multivariate time series analysis.\n",
    "\n",
    "Autoregressive models use lagged past values of the variable and have an order based on how many times the variable has been lagged, i.e. VAR(2) means two lagged values. Lagged values help to determine if seasonality or reoccurring patterns exist in the data. \n",
    "\n",
    "A pth-order VAR model is written as:\n",
    "$$y_t = c + A_1 y_{t-1} + A_2 y_{t-2} + \\cdots + A_p y_{t-p} + e_t,$$\n",
    "\n",
    "In vector notation, a VAR(1) with 2 variables is:\n",
    "\n",
    "$$\\begin{bmatrix}y_{1,t} \\\\ y_{2,t}\\end{bmatrix} = \\begin{bmatrix}c_{1} \\\\ c_{2}\\end{bmatrix} + \\begin{bmatrix}a_{1,1}&a_{1,2} \\\\ a_{2,1}&a_{2,2}\\end{bmatrix}\\begin{bmatrix}y_{1,t-1} \\\\ y_{2,t-1}\\end{bmatrix} + \\begin{bmatrix}e_{1,t} \\\\ e_{2,t}\\end{bmatrix}$$\n",
    "\n",
    "For our VAR model, we will have a vector of gas signals.\n",
    "\n",
    "#### Assumptions:\n",
    "* Every error term has a mean of zero and is normally distributed.\n",
    "* Variables are stationary. Differencing can be used to make the data stationary.\n",
    "* Data is on the same scale\n",
    "* Time sampling is consistent \n",
    "\n",
    "### Other methods with fewer assumptions\n",
    "\n",
    "Non-parametric Bayesian VAR models have been under development and appear to perform well and can operate on nonlinear relationships, heteroscedasticity, and non-Gaussian error data[1,2].\n",
    "\n",
    "Another popular time series analysis modeling technique is the Autoregressive Integrated Moving Average (ARIMA) model. These models are often used in forecasting or when the data shows evidence of non-stationarity. For multiple time series vectors, as we have here, extensions of the ARIMA model are available, such ARIMAX model that has exogenous variable inputs. ARIMAX models do not have an assumption of Gaussian errors as it uses a maximum likelihood estimation function to fit.\n",
    "\n",
    "#### References\n",
    "- 1 Kalli, Maria & Griffin, Jim E., 2018. \"Bayesian nonparametric vector autoregressive models,\" Journal of Econometrics, Elsevier, vol. 203(2), pages 267-282.\n",
    "- 2 Jeliazkov, Ivan. (2013). Nonparametric Vector Autoregressions: Specification, Estimation, and Inference. Advances in Econometrics. 33. 10.1108/S0731-9053(2013)0000031009. \n",
    "\n",
    "\n",
    "We have taken the trade-off in assumption for this first version correlation model vs a non-parametric approach for simplicity's sake, that current robust python VAR implementations exist, and for illustrative purposes around the concepts, we are addressing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from statsmodels.tsa.api import VAR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load connection string\n",
    "CONN_STRING_PATH = 'sentinel_conn_string_andrew.txt'\n",
    "\n",
    "with open(CONN_STRING_PATH, 'r') as fid:\n",
    "    conn_string = fid.read()\n",
    "    \n",
    "# create database connection.\n",
    "connection = create_engine(conn_string, pool_recycle=3600).connect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data EDA\n",
    "\n",
    "Below we download hourly averages from the `messages` and `mesage_gas_economy` table from May 1st, 2021 to present (last refreshed 6/28/2021). After downloading the data, we few the first and lasts 5 rows, and perform basic statistics on the data.\n",
    "\n",
    "\n",
    "### Data Dictionary - coped from [Sentinel's Data Dictionary](https://github.com/filecoin-project/sentinel/blob/master/docs/db.md)\n",
    "## `message_gas_economy`\n",
    "Gas economics for all messages in all blocks at each epoch.\n",
    "\n",
    "|Name|Type|Nullable|Description|\n",
    "|---|---|---|---|\n",
    "|`base_fee`|`double precision`|NO|The set price per unit of gas (measured in attoFIL/gas unit) to be burned (sent to an unrecoverable address) for every message execution.|\n",
    "|`base_fee_change_log`|`double precision`|NO|The logarithm of the change between new and old base fee.|\n",
    "|`gas_capacity_ratio`|`double precision`|YES|The gas_limit_unique_total / target gas limit total for all blocks.|\n",
    "|`gas_fill_ratio`|`double precision`|YES|The gas_limit_total / target gas limit total for all blocks.|\n",
    "|`gas_limit_total`|`bigint`|NO|The sum of all the gas limits.|\n",
    "|`gas_limit_unique_total`|`bigint`|YES|The sum of all the gas limits of unique messages.|\n",
    "|`gas_waste_ratio`|`double precision`|YES|(gas_limit_total - gas_limit_unique_total) / target gas limit total for all blocks.|\n",
    "|`height`|`bigint`|NO|Epoch these economics apply to.|\n",
    "|`state_root`|`text`|NO|CID of the parent state root at this epoch.|\n",
    "\n",
    "\n",
    "## `messages`\n",
    "Validated on-chain messages by their CID and their metadata.\n",
    "\n",
    "|Name|Type|Nullable|Description|\n",
    "|---|---|---|---|\n",
    "|`cid`|`text`|NO|CID of the message.|\n",
    "|`from`|`text`|NO|Address of the actor that sent the message.|\n",
    "|`gas_fee_cap`|`text`|NO|The maximum price that the message sender is willing to pay per unit of gas.|\n",
    "|`gas_limit`|`bigint`|NO|-|\n",
    "|`gas_premium`|`text`|NO|The price per unit of gas (measured in attoFIL/gas) that the message sender is willing to pay (on top of the BaseFee) to \"tip\" the miner that will include this message in a block.|\n",
    "|`height`|`bigint`|NO|Epoch this message was executed at.|\n",
    "|`method`|`bigint`|YES|The method number invoked on the recipient actor. Only unique to the actor the method is being invoked on. A method number of 0 is a plain token transfer - no method exectution.|\n",
    "|`nonce`|`bigint`|NO|The message nonce, which protects against duplicate messages and multiple messages with the same values.|\n",
    "|`size_bytes`|`bigint`|NO|Size of the serialized message in bytes.|\n",
    "|`to`|`text`|NO|Address of the actor that received the message.|\n",
    "|`value`|`text`|NO|Amount of FIL (in attoFIL) transferred by this message.|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "QUERY = \"\"\"\n",
    "SELECT \n",
    "*\n",
    "FROM messages m\n",
    "WHERE\n",
    "to_timestamp(height_to_unix(m.height)) > '2021-06-01'\n",
    "\"\"\"\n",
    "messages = (pd.read_sql(QUERY, connection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "derived_gas_outputs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derived_gas_outputs.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "derived_gas_outputs.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will plot each signal, save for the timestamp, below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in derived_gas_outputs.columns[1:]:\n",
    "    derived_gas_outputs.plot(x='timestamp',y=i,kind='line',title=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is pretty consistent, stochastic trend save for a several orders of magnitude spike one June 3rd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derived_gas_outputs.query('mean_gas_burned > 4000000')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Messages calculation mapping\n",
    "\n",
    "Based on the message cost calculation outliend by Filecoin's official documentation, we will map the data obtained to this calculation.\n",
    "\n",
    "Filecoin:\n",
    "`message_cost_calculation =  GasUsage * BaseFee FIL (burned) + GasLimit * GasPremium FIL (miner reward) + OverEstimationBurn * BaseFee FIL`\n",
    "\n",
    "\n",
    "Our downloaded data:\n",
    "`message_cost = derived_gas_outputs.mean_gas_used * derived_gas_outputs.mean_base_fee_burn + derived_gas_outputs.mean_gas_limit * derived_gas_outputs.mean_gas_premium + derived_gas_outputs.mean_over_estimation_burn * derived_gas_outputs.mean_base_fee_burn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create field\n",
    "derived_gas_outputs['mean_message_cost'] = derived_gas_outputs.mean_gas_used * derived_gas_outputs.mean_base_fee_burn + derived_gas_outputs.mean_gas_limit * derived_gas_outputs.mean_gas_premium + derived_gas_outputs.mean_over_estimation_burn * derived_gas_outputs.mean_base_fee_burn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "derived_gas_outputs.plot(x='timestamp',y='mean_message_cost',kind='line',title='Mean Message Cost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing\n",
    "\n",
    "We will now remove the timestamp field and examine the data distributions and determine if any transformations are required prior to our VAR modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = derived_gas_outputs.timestamp\n",
    "del derived_gas_outputs['timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derived_gas_outputs.hist(bins=30, figsize=(15, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the above historgrams that besides `mean_gas_used` and `mean_gas_refund` our data is not normally distributed and will need to be transformed prior to modeling. We will take the log of the data to reduce the skewness and take the first difference to make the data stationary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "log_differenced = pd.DataFrame()\n",
    "## Difference and log values\n",
    "for i in derived_gas_outputs.columns:\n",
    "    log_differenced[i +'_log_differenced'] = np.log(derived_gas_outputs[i]).diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_differenced.hist(bins=30, figsize=(15, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ### Data Transformations\n",
    "\n",
    "We will take the log of the data to reduce the skewness and take the first difference to make the data stationary, meaning that the distribution does not change when shifted by tie. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace any NA values with zero\n",
    "log_differenced.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final check prior to modeling, we will run the Augmented Dickey-Fuller test to ensure that our data is stationary (non-unit root - A unit root is a stochastic trend in a time series). The test's hypothesis are:\n",
    "\n",
    "\n",
    "* $H_O: \\textrm{Time series has a unit root}$\n",
    "* $H_A: \\textrm{Time series does not have a unit root}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in log_differenced.columns:\n",
    "    print(i)\n",
    "    result = adfuller(log_differenced[i].values)\n",
    "    print('ADF Statistic: %f' % result[0])\n",
    "    print('p-value: %f' % result[1])\n",
    "    if result[1] > 0.05:\n",
    "        decision = \"fail to reject - unit root present\"\n",
    "    else:\n",
    "        decision = \"reject, no unit root present\"\n",
    "    print(decision)\n",
    "    print('----------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the Augmented Dickey-Fuller, our preprocessing was successful and none of our univariate time series signals as a unit root. We can now proceed to the VAR model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fit\n",
    "\n",
    "To determine the ideal number of lags for our model, we will perform a heuristic SVD. We will fit our model with an autocorrelation between 1 and 15 to ascertain which VAR order has the best Akaike information criterion(AIC) score.\n",
    "\n",
    "The **Akaike information criterion (AIC)** is an estimator of prediction error, rooted in information theory. Given a collection of models for the data, AIC estimates the quality of each model, relative to each of the other models as a means for model selection.\n",
    "\n",
    "When a statistical model is used to represent the process that generated the data, the representation will rarely be exact; so some information will be lost by using the model to represent the process. AIC estimates the relative amount of information lost by a given model: the less information a model loses, the higher the quality of that model.\n",
    "\n",
    "In estimating the amount of information lost by a model, AIC deals with the trade-off between the goodness of fit of the model and the simplicity of the model. In other words, AIC deals with both the risk of overfitting and the risk of underfitting.\n",
    "\n",
    "Below is the equation for AIC where $\\hat L$ is the maximum value of the likelihood function for the model: \n",
    "\n",
    "$$\\mathrm{AIC} \\, = \\, 2k - 2\\ln(\\hat L)$$\n",
    "\n",
    "Given a set of candidate models for the data, the **preferred model is the one with the minimum AIC value, the sign of the data does not matter**. AIC optimizes for the goodness of fit but also includes a penalty for each additional parameter, which discourages overfitting.\n",
    "\n",
    "<!-- The **Bayesian information criterion (BIC)** is a statistical method for model selection among a group of models; **the model with the lowest BIC is preferred, the sign of the data does not matter**. It is based, in part, on the likelihood function and it is closely related to the Akaike information criterion (AIC).\n",
    "\n",
    "When fitting models, it is possible to increase the likelihood by adding parameters, but doing so may result in overfitting. Both BIC and AIC attempt to resolve this problem by introducing a penalty term for the number of parameters in the model; the penalty term is larger in BIC than in AIC.\n",
    "\n",
    "$$\\mathrm{BIC} = k\\ln(n) - 2\\ln(\\widehat L)$$\n",
    "where\n",
    "$\\hat L$ = the maximized value of the likelihood function of the model $M$\n",
    "$x$ = the observed data;\n",
    "$n$ = the number of data points in $x$, \n",
    "$k$ = the number of parameters estimated by the model. \n",
    "\n",
    "\n",
    "Several researchers that have studied the two metrics argue that BIC is appropriate for selecting the \"true model\" (i.e. the process that generated the data) from the set of candidate models, whereas AIC choices the best model amongst the alternatives. Proponents of AIC argue that this issue is negligible because the \"true model\" is virtually never in the candidate set. For our use cases, we will optimize for the AIC as we know the \"true\" model of the signal behavior is way too complex to be captured within a VAR model.\n",
    " -->\n",
    "Paraphrased sources:\n",
    "* https://en.wikipedia.org/wiki/Akaike_information_criterion\n",
    "<!-- * https://en.wikipedia.org/wiki/Bayesian_information_criterion -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aic = []\n",
    "for i in range(1,16):\n",
    "    model = VAR(log_differenced)\n",
    "    results = model.fit(i)\n",
    "    aic.append(results.aic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(aic, 'r+')\n",
    "plt.legend(['AIC'])\n",
    "plt.xlabel('Autocorrelation Lag')\n",
    "plt.ylabel('AIC')\n",
    "plt.title('Plot of sweeps over lag depths over AIC Loss function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on our analysis, a lag of 4 appears to be the optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# instantiate the var model object from statsmodels\n",
    "model = VAR(log_differenced)\n",
    "\n",
    "# fit model with determined lag values\n",
    "results = model.fit(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Granger causality\n",
    "\n",
    "Granger causality is a hypothesis test for determining whether one-time series is useful in forecasting another. We can say that a variable X, or variables, evolves Granger-causes another evolving variable Y if predictions of the value of Y based on its past values and the past values of X are better than predictions of Y based only on Y's past values. \n",
    "\n",
    "Granger Causality is relationship based on the following principles:\n",
    "\n",
    "* The cause happens before its effect.\n",
    "* The cause has 'unique' information about the future values of its effect.\n",
    "\n",
    "\n",
    "Given these two assumptions about causality, Granger proposed to test the following hypothesis for identification of a causal effect of $X$ on $Y$:\n",
    "$${P}[Y(t+1) \\in A\\mid \\mathcal{I}(t)] \\neq \\mathbb{P}[Y(t+1) \\in A\\mid \\mathcal{I}_{-X}(t)]$$\n",
    "where $\\mathbb{P}$ refers to probability, $A$ is an arbitrary non-empty set, and $\\mathcal{I}(t)$ and $\\mathcal{I}_{-X}(t)$ respectively denote the information available as of time $t$ in the entire universe, and that in the modified universe in which $X$ is excluded. If the above hypothesis is accepted, we say that $X$ Granger causes $Y$.\n",
    "\n",
    "In our analysis, we present the hypothesis that gas_used is a driver of message cost. In statistical parlance, we have the following:\n",
    "\n",
    "* $H_O: \\textrm{Gas used does not Granger cause message cost}$\n",
    "* $H_A: \\textrm{Gas used does Granger cause message cost}$\n",
    "\n",
    "\n",
    "Granger Causality assumes that the time series are non-stationary, which we checked and passed above, and autoregressive lags greater than 1.\n",
    "\n",
    "We will perform now perform the Granger Causality hypothesis test with an $\\alpha = 0.05$ value using an F test to determine if the eth price has any casual component for predicting the Rai market price. If the p-value (the probability of obtaining test results at least as extreme as the results observed) of the test is less than or equal to $\\alpha$ we will reject the null hypothesis and determine that gas used is a driver of message cost.\n",
    "\n",
    "### Paraphrased source:\n",
    "* https://en.wikipedia.org/wiki/Granger_causality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have many signals with the analysis, we will loop through all the signals, perform the Granger Causality test, and save the results for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "result_dfs = []\n",
    "variables = list(log_differenced.columns)\n",
    "for i in variables:\n",
    "    for j in variables:\n",
    "        if i==j:\n",
    "            pass\n",
    "        else:\n",
    "            results_summary = results.test_causality(i,j, \n",
    "                                                     kind='f',signif=alpha).summary()\n",
    "\n",
    "\n",
    "            df = pd.read_html(results_summary.as_html(),header=0, index_col=0)[0]\n",
    "            df['alpha'] = alpha\n",
    "            df['caused'] = i\n",
    "            df['causing'] = j\n",
    "            result_dfs.append(df)\n",
    "        \n",
    "Grangers_df = pd.concat(result_dfs)\n",
    "Grangers_df.reset_index(inplace=True)\n",
    "Grangers_df['result'] = Grangers_df['p-value'].apply(lambda x: 'reject H0' if x < alpha else 'fail to reject H0' )\n",
    "Grangers_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "heatdf = Grangers_df.pivot(index='caused', columns=['causing'],values=['p-value'])\n",
    "\n",
    "f, ax = plt.subplots(figsize=(15, 12))\n",
    "sns.heatmap(heatdf, annot=True, linewidths=.5, ax=ax,cmap=['green','red'],center=0.05)\n",
    "plt.suptitle('Heatmap of Granger Casuality \\n Green shows causal relationship whereas red shows no causal relationship')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the heatmap above, we can see that there are some granger causal relationships between signals. To see more granular specifics, we will examine the reject H0s below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Grangers_df.query(\"result == 'reject H0'\").sort_values('p-value',ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above table, we can see some Granger caused relationships between signals, such as mean_gas_fee_cap Granger causes mean_gas_limit. We can't fully use Granger until we understand what the signals are and what they represent better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion \n",
    "In this notebook, we've provided proof of workflow for using VAR and Granger Causality to analyze the relationships between variables. In a subsequent notebook, we will examine the gas data more deeply to answer the following questions to use gained insight to improve Filecoin:\n",
    "\n",
    "* Understand data we have by performing a data baseline - event and aggregated data.\n",
    "* Reverse engineer June 3 spike - what happened and why? Can we generalize what drives burning?\n",
    "* Distribution of attributes of things people are willing to pay gas fees for. What types of transactions are individuals paying more for?\n",
    "* Are revenues over cost?\n",
    "* Create curated data signals for Digital Twin use\n",
    "* Obtain message metadata, try and predict cost from it (classifier or regressor). Which attributes are mapped to smaller or larger costs? Which messages are worth paying a lot for? The question is what people are they paying for and *why*?\n",
    "* Reperform Granger Causality when we have more data understanding\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
